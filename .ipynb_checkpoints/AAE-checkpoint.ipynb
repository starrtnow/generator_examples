{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = torchvision.datasets.MNIST(root = \"./MNIST\", download=True, transform= transforms.ToTensor())\n",
    "dataloader = torch.utils.data.DataLoader(dataset=datasets, shuffle=True, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, height, width, n_channel = 1, n_hidden = 500, z_dim = 300):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.z_dim = z_dim\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_channel = n_channel\n",
    "        self.lin1 = nn.Linear(width * height * n_channel, n_hidden)\n",
    "        \n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, z_dim))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = X.view(-1, self.width * self.height * self.n_channel)\n",
    "        out = F.relu(self.lin1(X))\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "    \n",
    "    def sample(self, batch_size, mu, log_var):\n",
    "        eps = Variable(torch.randn(batch_size, self.z_dim)).cuda()\n",
    "        return mu + torch.exp(log_var / 2) * eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, width, height, n_channel = 1, n_hidden = 500, z_dim = 300):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.z_dim = z_dim\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_channel = n_channel\n",
    "        \n",
    "        self.lin1 = nn.Sequential(\n",
    "            nn.Linear(z_dim, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden))\n",
    "        \n",
    "        self.out = nn.Linear(n_hidden, self.width * self.height * n_channel)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = F.relu(self.lin1(X))\n",
    "        out = F.sigmoid(self.out(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, z_dim, n_hidden = 500):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z_dim, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = self.net(X)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "width = 28\n",
    "height = 28\n",
    "encoder = Encoder(width, height, z_dim=5, n_hidden=1000)\n",
    "decoder = Decoder(width, height, z_dim=5, n_hidden=1000)\n",
    "discriminator = Discriminator(encoder.z_dim)\n",
    "TINY = 1e-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_lr, reg_lr = 0.0006, 0.0003\n",
    "autoenc_lr, gen_disc_lr = 0.01, 0.1\n",
    "decoding_optim = torch.optim.Adam(decoder.parameters(), lr=autoenc_lr)\n",
    "encoder_encoding_optim = torch.optim.Adam(encoder.parameters(), lr=autoenc_lr)\n",
    "encoder_generator_optim = torch.optim.Adam(encoder.parameters(), lr=gen_disc_lr)\n",
    "discriminator_optim = torch.optim.Adam(discriminator.parameters(),lr=gen_disc_lr)\n",
    "\n",
    "def turn_reg_off():\n",
    "    encoder_encoding_optim = torch.optim.Adam(encoder.parameters(), lr=0)\n",
    "    discriminator_optim = torch.optim.Adam(discriminator.parameters(), lr=0)\n",
    "\n",
    "def turn_reg_on():\n",
    "    encoder_generator_optim = torch.optim.Adam(encoder.parameters(), lr=reg_lr)\n",
    "    discriminator_optim = torch.optim.Adam(discriminator.parameters(), lr=reg_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator (\n",
       "  (net): Sequential (\n",
       "    (0): Linear (5 -> 500)\n",
       "    (1): ReLU ()\n",
       "    (2): Linear (500 -> 500)\n",
       "    (3): ReLU ()\n",
       "    (4): Linear (500 -> 1)\n",
       "    (5): Sigmoid ()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#enable cuda\n",
    "encoder.cuda()\n",
    "decoder.cuda()\n",
    "discriminator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def zero_grads():\n",
    "    encoder.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "    discriminator.zero_grad()\n",
    "    \n",
    "def run_training_epoch():\n",
    "    for img, label in dataloader:\n",
    "        \n",
    "        \n",
    "        \n",
    "        zero_grads()\n",
    "        \n",
    "        img = img * 0.3081 + 0.1307\n",
    "        X = Variable(img.cuda())\n",
    "        z = encoder(X)\n",
    "        X_recon = decoder(z)\n",
    "        recon_loss = F.binary_cross_entropy(X_recon + TINY, X + TINY)\n",
    "\n",
    "        recon_loss.backward()\n",
    "        decoding_optim.step()\n",
    "        encoder_encoding_optim.step()\n",
    "        \n",
    "        \n",
    "        #Regularization\n",
    "        zero_grads()\n",
    "\n",
    "        z_real_gauss = Variable((torch.randn(200, encoder.z_dim) * 5.0).cuda())\n",
    "        z_fake_gauss = encoder(X)\n",
    "\n",
    "        real_gauss_loss = discriminator(z_real_gauss)\n",
    "        fake_gauss_loss = discriminator(z_fake_gauss)\n",
    "        \n",
    "        #print(real_gauss_loss)\n",
    "        #print(fake_gauss_loss)\n",
    "        \n",
    "        total_disc_loss = -torch.mean(torch.log(real_gauss_loss + TINY) + torch.log(1 - fake_gauss_loss + TINY))\n",
    "        total_disc_loss.backward()\n",
    "        discriminator_optim.step()\n",
    "        \n",
    "       \n",
    "        #Generator\n",
    "        zero_grads()\n",
    "        \n",
    "        encoder_generator_optim.step()\n",
    "        z_fake = encoder(X)\n",
    "        z_fake_disc = discriminator(z_fake)\n",
    "            \n",
    "        g_loss = -torch.mean(torch.log(z_fake_disc + TINY))\n",
    "        g_loss.backward()\n",
    "        encoder_generator_optim.step()\n",
    "        \n",
    "        zero_grads()\n",
    "       \n",
    "    return recon_loss.data[0], total_disc_loss.data[0], g_loss.data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ad5775ebf53e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mturn_reg_on\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_training_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Epoch {}, loss {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtest_z\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-5799dabc30b8>\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mtotal_disc_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_gauss_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mTINY\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfake_gauss_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mTINY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mtotal_disc_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mdiscriminator_optim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\envs\\PYT\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "for epoch in range(n_epochs):\n",
    "    losses = run_training_epoch()\n",
    "    print(\"Epoch {}, loss {}\".format(epoch, losses))\n",
    "    test_z = Variable(torch.FloatTensor(200, encoder.z_dim).normal_()).cuda()\n",
    "    result = decoder(test_z).data.view(-1, 1, 28, 28)\n",
    "    torchvision.utils.save_image(result, \"aae-results-{}.png\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "test_z = Variable(torch.FloatTensor(200, encoder.z_dim).normal_()).cuda()\n",
    "result = decoder(test_z).data.view(-1, 1, 28, 28)\n",
    "torchvision.utils.save_image(result, \"vae-results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
